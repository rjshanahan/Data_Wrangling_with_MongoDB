{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###Lesson 1 : EXERCISE 1\n",
      "import os\n",
      "\n",
      "DATADIR = \"/Users/richardshanahan/Documents/DATA SCIENCE/99. Self Study/Udacity/MongoDB Data Wrangling/99. Downloads/\"\n",
      "DATAFILE = \"beatles-diskography.csv\"\n",
      "\n",
      "\n",
      "data = []\t\t\t\t\t\t\t\t\t\t\t\t\t#empty list\n",
      "    \t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "with open(datafile, \"r\") as f:\t\t\t\t\t\t\t\t#datafile defined in 'test' function\n",
      "    headerkeys = f.readline().split(\",\")\t\t\t\t\t#define keys from header row\n",
      "    counter = 0\t\t\t\t\t\t\t\t\t\t\t\t#start point for counting\t\t\t\n",
      "        \n",
      "    for line in f:\n",
      "        if counter == 10:\t\t\t\t\t\t\t\t\t#stop loop at 10 iterations\n",
      "            break\n",
      "            \n",
      "        bodyvalues = line.split(\",\")\t\t\t\t\t\t#values for dict\n",
      "        minidicts = {}\t\t\t\t\t\t\t\t\t\t#empty dict to write to\n",
      "            \n",
      "        for i, value in enumerate(bodyvalues):\t\t\t\t#enumerate provides an index\n",
      "            minidicts[headerkeys[i].strip()] = value.strip() #loop through split list for key:value\n",
      "                \n",
      "        data.append(minidicts)\t\t\t\t\t\t\t\t#write key:value pairs to list\n",
      "        counter += 1\t\t\t\t\t\t\t\t\t\t#adds right operand to the left operand and assign the result to left operand\n",
      "            \n",
      "print data\t\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}, {'Title': 'With the Beatles', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 November 1963', 'US Chart Position': '-', 'RIAA Certification': 'Gold', 'BPI Certification': 'Platinum'}, {'Title': 'Beatlemania! With the Beatles', 'UK Chart Position': '-', 'Label': 'Capitol(CAN)', 'Released': '25 November 1963', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': ''}, {'Title': 'Introducing... The Beatles', 'UK Chart Position': '-', 'Label': 'Vee-Jay(US)', 'Released': '10 January 1964', 'US Chart Position': '2', 'RIAA Certification': '', 'BPI Certification': ''}, {'Title': 'Meet the Beatles!', 'UK Chart Position': '-', 'Label': 'Capitol(US)', 'Released': '20 January 1964', 'US Chart Position': '1', 'RIAA Certification': '5xPlatinum', 'BPI Certification': ''}, {'Title': 'Twist and Shout', 'UK Chart Position': '-', 'Label': 'Capitol(CAN)', 'Released': '3 February 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': ''}, {'Title': \"The Beatles' Second Album\", 'UK Chart Position': '-', 'Label': 'Capitol(US)', 'Released': '10 April 1964', 'US Chart Position': '1', 'RIAA Certification': '2xPlatinum', 'BPI Certification': ''}, {'Title': \"The Beatles' Long Tall Sally\", 'UK Chart Position': '-', 'Label': 'Capitol(CAN)', 'Released': '11 May 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': ''}, {'Title': \"A Hard Day's Night\", 'UK Chart Position': '-', 'Label': 'United Artists(US)[C]', 'Released': '26 June 1964', 'US Chart Position': '1', 'RIAA Certification': '4xPlatinum', 'BPI Certification': ''}, {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Lesson 1 :EXERCISE 2: list comprehension and how to work/view data in xlrd package\n",
      "\n",
      "#pip install xlrd\n",
      "\n",
      "import xlrd\n",
      "\n",
      "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
      "\n",
      "\n",
      "def parse_file(datafile):\n",
      "    workbook = xlrd.open_workbook(datafile)\n",
      "    sheet = workbook.sheet_by_index(0)\n",
      "\n",
      "    data = [[sheet.cell_value(r, col) \n",
      "                for col in range(sheet.ncols)] \n",
      "                    for r in range(sheet.nrows)]\n",
      "\n",
      "    print \"\\nList Comprehension\"\n",
      "    print \"data[3][2]:\",\n",
      "    print data[3][2]\n",
      "\n",
      "    print \"\\nCells in a nested loop:\"    \n",
      "    for row in range(sheet.nrows):\n",
      "        for col in range(sheet.ncols):\n",
      "            if row == 50:\n",
      "                print sheet.cell_value(row, col),\n",
      "\n",
      "\n",
      "    ### other useful methods:\n",
      "    print \"\\nROWS, COLUMNS, and CELLS:\"\n",
      "    print \"Number of rows in the sheet:\", \n",
      "    print sheet.nrows\n",
      "    print \"Type of data in cell (row 3, col 2):\",         #will return a number that indicated a datatype in xlrd\n",
      "    print sheet.cell_type(3, 2)\n",
      "    print \"Value in cell (row 3, col 2):\", \n",
      "    print sheet.cell_value(3, 2)\n",
      "    print \"Get a slice of values in column 3, from rows 1-3:\"\n",
      "    print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
      "\n",
      "    print \"\\nDATES:\"\n",
      "    print \"Type of data in cell (row 1, col 0):\", \n",
      "    print sheet.cell_type(1, 0)\n",
      "    exceltime = sheet.cell_value(1, 0)\n",
      "    print \"Time in Excel format:\",\n",
      "    print exceltime\n",
      "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
      "    print xlrd.xldate_as_tuple(exceltime, 0)\n",
      "\n",
      "    return data\n",
      "\n",
      "data = parse_file(datafile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "List Comprehension\n",
        "data[3][2]: 1036.088697\n",
        "\n",
        "Cells in a nested loop:\n",
        "41277.0833333 9238.73731 1438.20528 1565.442856 916.708348 14010.903488 3027.98334 6165.211119 1157.741663 37520.933404 \n",
        "ROWS, COLUMNS, and CELLS:\n",
        "Number of rows in the sheet: 7296\n",
        "Type of data in cell (row 3, col 2): 2\n",
        "Value in cell (row 3, col 2): 1036.088697\n",
        "Get a slice of values in column 3, from rows 1-3:\n",
        "[1411.7505669999982, 1403.4722870000019, 1395.053150000001]\n",
        "\n",
        "DATES:\n",
        "Type of data in cell (row 1, col 0): 3\n",
        "Time in Excel format: 41275.0416667\n",
        "Convert time to a Python datetime tuple, from the Excel float: (2013, 1, 1, 1, 0, 0)\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Lesson 1 :Exercise 3: using xlrd package\n",
      "\n",
      "import numpy as np\n",
      "import pprint\n",
      "\n",
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Your task is as follows:\n",
      "- read the provided Excel file\n",
      "- find and return the min, max and average values for the COAST region\n",
      "- find and return the time value for the min and max entries\n",
      "- the time values should be returned as Python tuples\n",
      "\n",
      "Please see the test function for the expected return format\n",
      "\"\"\"\n",
      "\n",
      "import xlrd\n",
      "from zipfile import ZipFile\n",
      "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
      "\n",
      "\n",
      "def open_zip(datafile):\n",
      "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
      "        myzip.extractall()\n",
      "\n",
      "\n",
      "def parse_file(datafile):\n",
      "    workbook = xlrd.open_workbook(datafile)\n",
      "    sheet = workbook.sheet_by_index(0)\n",
      "    global data\n",
      "\n",
      "    ### example on how you can get the data\n",
      "    #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
      "\n",
      "    ### other useful methods:\n",
      "    # print \"\\nROWS, COLUMNS, and CELLS:\"\n",
      "    # print \"Number of rows in the sheet:\", \n",
      "    # print sheet.nrows\n",
      "    # print \"Type of data in cell (row 3, col 2):\", \n",
      "    # print sheet.cell_type(3, 2)\n",
      "    # print \"Value in cell (row 3, col 2):\", \n",
      "    # print sheet.cell_value(3, 2)\n",
      "    # print \"Get a slice of values in column 3, from rows 1-3:\"\n",
      "    # print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
      "\n",
      "    # print \"\\nDATES:\"\n",
      "    # print \"Type of data in cell (row 1, col 0):\", \n",
      "    # print sheet.cell_type(1, 0)\n",
      "    # exceltime = sheet.cell_value(1, 0)\n",
      "    # print \"Time in Excel format:\",\n",
      "    # print exceltime\n",
      "    # print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
      "    # print xlrd.xldate_as_tuple(exceltime, 0)\n",
      "    \n",
      "    \n",
      "    #read time column\n",
      "    times_float = sheet.col_values(0, start_rowx=1)\n",
      "\n",
      "    #read the 'Coast' column\n",
      "    coast = sheet.col_values(1,start_rowx=1)\n",
      "\n",
      "    #index of the max/min coast\n",
      "    idx_max = coast.index(max(coast))\n",
      "    idx_min = coast.index(min(coast))\n",
      "    \n",
      "    \n",
      "    data = {#'maxtime': xlrd.xldate_as_tuple(max(sheet.col_values(0, start_rowx=1)), 0),\n",
      "            'maxvalue': round(max(sheet.col_values(1, start_rowx=1)), 10),\n",
      "            'maxtime': xlrd.xldate_as_tuple(times_float[idx_max],0),\n",
      "            #'mintime': xlrd.xldate_as_tuple(min(sheet.col_values(0, start_rowx=1)), 0),\n",
      "            'minvalue': round(min(sheet.col_values(1, start_rowx=1)), 10),\n",
      "            'mintime': xlrd.xldate_as_tuple(times_float[idx_min],0),\n",
      "            'avgcoast': round(float(sum(sheet.col_values(1, start_rowx=1)))/len(sheet.col_values(1, start_rowx=1)) if len(sheet.col_values(1, start_rowx=1)) > 0 else float('nan'), 10)}\n",
      "    \n",
      "    return data\n",
      "\n",
      "\n",
      "def test():\n",
      "    #open_zip(datafile)\n",
      "    data = parse_file(datafile)\n",
      "    pprint.pprint(data)\n",
      "\n",
      "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
      "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
      "    \n",
      "\n",
      "\n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'avgcoast': 10976.9334606798,\n",
        " 'maxtime': (2013, 8, 13, 17, 0, 0),\n",
        " 'maxvalue': 18779.02551,\n",
        " 'mintime': (2013, 2, 3, 4, 0, 0),\n",
        " 'minvalue': 6602.113899}\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Lesson 1 :Exercise 3 their solution\n",
      "\n",
      "\n",
      "##Exercise 3: using xlrd package\n",
      "\n",
      "import numpy as np\n",
      "import pprint\n",
      "\n",
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Your task is as follows:\n",
      "- read the provided Excel file\n",
      "- find and return the min, max and average values for the COAST region\n",
      "- find and return the time value for the min and max entries\n",
      "- the time values should be returned as Python tuples\n",
      "\n",
      "Please see the test function for the expected return format\n",
      "\"\"\"\n",
      "\n",
      "import xlrd\n",
      "from zipfile import ZipFile\n",
      "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
      "\n",
      "\n",
      "def open_zip(datafile):\n",
      "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
      "        myzip.extractall()\n",
      "\n",
      "\n",
      "def parse_file(datafile):\n",
      "    workbook = xlrd.open_workbook(datafile)\n",
      "    sheet = workbook.sheet_by_index(0)\n",
      "    global data\n",
      "\n",
      "    ### example on how you can get the data\n",
      "    data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
      "\n",
      "    cv = sheet.col_values(1, start_rowx=1, end_rowx=None)\n",
      "    \n",
      "    maxval = max(cv)\n",
      "    minval = min(cv)\n",
      "    \n",
      "    maxpos = cv.index(maxval) + 1\n",
      "    minpos = cv.index(minval) + 1\n",
      "    \n",
      "    maxtime = sheet.cell_value(maxpos, 0)\n",
      "    realtime = xlrd.xldate_as_tuple(maxtime,0)\n",
      "    mintime = sheet.cell_value(minpos, 0)\n",
      "    realmintime = xlrd.xldate_as_tuple(mintime,0)\n",
      "    \n",
      "    \n",
      "    data = {'maxvalue': maxval,\n",
      "            'maxtime': realtime,\n",
      "            'minvalue': minval,\n",
      "            'mintime': realmintime,\n",
      "            'avgcoast': round(float(sum(sheet.col_values(1, start_rowx=1)))/len(sheet.col_values(1, start_rowx=1)) if len(sheet.col_values(1, start_rowx=1)) > 0 else float('nan'), 10)}\n",
      "    \n",
      "    \n",
      "    return data\n",
      "\n",
      "\n",
      "def test():\n",
      "    #open_zip(datafile)\n",
      "    data = parse_file(datafile)\n",
      "    pprint.pprint(data)\n",
      "\n",
      "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
      "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
      "    \n",
      "\n",
      "\n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'avgcoast': 10976.9334606798,\n",
        " 'maxtime': (2013, 8, 13, 17, 0, 0),\n",
        " 'maxvalue': 18779.025510000003,\n",
        " 'mintime': (2013, 2, 3, 4, 0, 0),\n",
        " 'minvalue': 6602.113898999982}\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###Lesson 1: Exercise 4 JSON webqueries - their example\n",
      "\n",
      "\n",
      "# To experiment with this code freely you will have to run this code locally.\n",
      "# We have provided an example json output here for you to look at,\n",
      "# but you will not be able to run any queries through our UI.\n",
      "\n",
      "import json\n",
      "import requests\n",
      "\n",
      "\n",
      "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
      "ARTIST_URL = BASE_URL + \"artist/\"\n",
      "\n",
      "query_type = {  \"simple\": {},\n",
      "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
      "                \"aliases\": {\"inc\": \"aliases\"},\n",
      "                \"releases\": {\"inc\": \"releases\"}}\n",
      "\n",
      "\n",
      "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
      "    params[\"fmt\"] = fmt\n",
      "    r = requests.get(url + uid, params=params)\n",
      "    print \"requesting\", r.url\n",
      "\n",
      "    if r.status_code == requests.codes.ok:\n",
      "        return r.json()\n",
      "    else:\n",
      "        r.raise_for_status()\n",
      "\n",
      "\n",
      "def query_by_name(url, params, name):\n",
      "    params[\"query\"] = \"artist:\" + name\n",
      "    return query_site(url, params)\n",
      "\n",
      "\n",
      "def pretty_print(data, indent=4):\n",
      "    if type(data) == dict:\n",
      "        print json.dumps(data, indent=indent, sort_keys=True)\n",
      "    else:\n",
      "        print data\n",
      "\n",
      "\n",
      "def main():\n",
      "    global artist_id\n",
      "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
      "    pretty_print(results)\n",
      "\n",
      "    artist_id = results[\"artists\"][1][\"id\"]\n",
      "    print \"\\nARTIST:\"\n",
      "    pretty_print(results[\"artists\"][1])\n",
      "\n",
      "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
      "    releases = artist_data[\"releases\"]\n",
      "    print \"\\nONE RELEASE:\"\n",
      "    pretty_print(releases[0], indent=2)\n",
      "    release_titles = [r[\"title\"] for r in releases]\n",
      "\n",
      "    print \"\\nALL TITLES:\"\n",
      "    for t in release_titles:\n",
      "        print t\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
        "{\n",
        "    \"artists\": [\n",
        "        {\n",
        "            \"aliases\": [\n",
        "                {\n",
        "                    \"begin-date\": null, \n",
        "                    \"end-date\": null, \n",
        "                    \"locale\": null, \n",
        "                    \"name\": \"Nirvana US\", \n",
        "                    \"primary\": null, \n",
        "                    \"sort-name\": \"Nirvana US\", \n",
        "                    \"type\": null\n",
        "                }\n",
        "            ], \n",
        "            \"area\": {\n",
        "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
        "                \"name\": \"United States\", \n",
        "                \"sort-name\": \"United States\"\n",
        "            }, \n",
        "            \"begin-area\": {\n",
        "                \"id\": \"a640b45c-c173-49b1-8030-973603e895b5\", \n",
        "                \"name\": \"Aberdeen\", \n",
        "                \"sort-name\": \"Aberdeen\"\n",
        "            }, \n",
        "            \"country\": \"US\", \n",
        "            \"disambiguation\": \"90s US grunge band\", \n",
        "            \"id\": \"5b11f4ce-a62d-471e-81fc-a69a8278c7da\", \n",
        "            \"life-span\": {\n",
        "                \"begin\": \"1988-01\", \n",
        "                \"end\": \"1994-04-05\", \n",
        "                \"ended\": true\n",
        "            }, \n",
        "            \"name\": \"Nirvana\", \n",
        "            \"score\": \"100\", \n",
        "            \"sort-name\": \"Nirvana\", \n",
        "            \"tags\": [\n",
        "                {\n",
        "                    \"count\": 2, \n",
        "                    \"name\": \"punk\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"legendary\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"90\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 2, \n",
        "                    \"name\": \"seattle\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"northwest\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"alternative\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"rock and indie\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"am\\u00e9ricain\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 2, \n",
        "                    \"name\": \"usa\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 7, \n",
        "                    \"name\": \"american\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"united states\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"kurt cobain\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 2, \n",
        "                    \"name\": \"90s\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 3, \n",
        "                    \"name\": \"alternative rock\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"nirvana\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"band\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 8, \n",
        "                    \"name\": \"rock\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 12, \n",
        "                    \"name\": \"grunge\"\n",
        "                }\n",
        "            ], \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
        "                \"name\": \"United Kingdom\", \n",
        "                \"sort-name\": \"United Kingdom\"\n",
        "            }, \n",
        "            \"begin-area\": {\n",
        "                \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
        "                \"name\": \"London\", \n",
        "                \"sort-name\": \"London\"\n",
        "            }, \n",
        "            \"country\": \"GB\", \n",
        "            \"disambiguation\": \"60s band from the UK\", \n",
        "            \"id\": \"9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6\", \n",
        "            \"life-span\": {\n",
        "                \"begin\": \"1967\", \n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Nirvana\", \n",
        "            \"score\": \"100\", \n",
        "            \"sort-name\": \"Nirvana\", \n",
        "            \"tags\": [\n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"rock\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"pop\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"progressive rock\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"orchestral\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"british\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"power pop\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"psychedelic rock\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"soft rock\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"symphonic rock\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"english\"\n",
        "                }\n",
        "            ], \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"6a264f94-6ff1-30b1-9a81-41f7bfabd616\", \n",
        "                \"name\": \"Finland\", \n",
        "                \"sort-name\": \"Finland\"\n",
        "            }, \n",
        "            \"country\": \"FI\", \n",
        "            \"disambiguation\": \"Early 1980's Finnish punk band\", \n",
        "            \"id\": \"85af0709-95db-4fbc-801a-120e9f4766d0\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Nirvana\", \n",
        "            \"score\": \"100\", \n",
        "            \"sort-name\": \"Nirvana\", \n",
        "            \"tags\": [\n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"punk\"\n",
        "                }, \n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"finland\"\n",
        "                }\n",
        "            ], \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"b305320e-c158-43f4-b5be-4450e2f99a32\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"El Nirvana\", \n",
        "            \"score\": \"62\", \n",
        "            \"sort-name\": \"Nirvana, El\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"329c04ae-3b73-4ca3-996f-75608ab1befb\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Nirvana Singh\", \n",
        "            \"score\": \"62\", \n",
        "            \"sort-name\": \"Singh, Nirvana\", \n",
        "            \"type\": \"Person\"\n",
        "        }, \n",
        "        {\n",
        "            \"aliases\": [\n",
        "                {\n",
        "                    \"begin-date\": null, \n",
        "                    \"end-date\": null, \n",
        "                    \"locale\": null, \n",
        "                    \"name\": \"Nirvana\", \n",
        "                    \"primary\": null, \n",
        "                    \"sort-name\": \"Nirvana\", \n",
        "                    \"type\": null\n",
        "                }, \n",
        "                {\n",
        "                    \"begin-date\": null, \n",
        "                    \"end-date\": null, \n",
        "                    \"locale\": null, \n",
        "                    \"name\": \"Prophet 2002\", \n",
        "                    \"primary\": null, \n",
        "                    \"sort-name\": \"Prophet 2002\", \n",
        "                    \"type\": null\n",
        "                }\n",
        "            ], \n",
        "            \"area\": {\n",
        "                \"id\": \"23d10872-f5ae-3f0c-bf55-332788a16ecb\", \n",
        "                \"name\": \"Sweden\", \n",
        "                \"sort-name\": \"Sweden\"\n",
        "            }, \n",
        "            \"country\": \"SE\", \n",
        "            \"disambiguation\": \"Swedish death metal band\", \n",
        "            \"id\": \"f2dfdff9-3862-4be0-bf85-9c833fa3059e\", \n",
        "            \"life-span\": {\n",
        "                \"begin\": \"1988\", \n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Nirvana 2002\", \n",
        "            \"score\": \"62\", \n",
        "            \"sort-name\": \"Nirvana 2002\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
        "                \"name\": \"United States\", \n",
        "                \"sort-name\": \"United States\"\n",
        "            }, \n",
        "            \"country\": \"US\", \n",
        "            \"id\": \"c3a64a25-251b-4d03-afba-1471440245b8\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Approaching Nirvana\", \n",
        "            \"score\": \"62\", \n",
        "            \"sort-name\": \"Approaching Nirvana\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
        "                \"name\": \"United States\", \n",
        "                \"sort-name\": \"United States\"\n",
        "            }, \n",
        "            \"country\": \"US\", \n",
        "            \"gender\": \"female\", \n",
        "            \"id\": \"206419e0-3a7a-49ce-8437-4e757767d02b\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Nirvana Savoury\", \n",
        "            \"score\": \"62\", \n",
        "            \"sort-name\": \"Savoury, Nirvana\", \n",
        "            \"type\": \"Person\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"86f9ae24-ba2a-4d55-9275-0b89b85f6e3a\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Weed Nirvana\", \n",
        "            \"score\": \"62\", \n",
        "            \"sort-name\": \"Weed Nirvana\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"e1388435-f80d-434a-9980-f1c9f5aa9b90\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Nirvana Sitar & String Group\", \n",
        "            \"score\": \"43\", \n",
        "            \"sort-name\": \"Nirvana Sitar & String Group\"\n",
        "        }\n",
        "    ], \n",
        "    \"count\": 10, \n",
        "    \"created\": \"2015-01-21T02:39:57.307Z\", \n",
        "    \"offset\": 0\n",
        "}\n",
        "\n",
        "ARTIST:\n",
        "{\n",
        "    \"area\": {\n",
        "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
        "        \"name\": \"United Kingdom\", \n",
        "        \"sort-name\": \"United Kingdom\"\n",
        "    }, \n",
        "    \"begin-area\": {\n",
        "        \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
        "        \"name\": \"London\", \n",
        "        \"sort-name\": \"London\"\n",
        "    }, \n",
        "    \"country\": \"GB\", \n",
        "    \"disambiguation\": \"60s band from the UK\", \n",
        "    \"id\": \"9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6\", \n",
        "    \"life-span\": {\n",
        "        \"begin\": \"1967\", \n",
        "        \"ended\": null\n",
        "    }, \n",
        "    \"name\": \"Nirvana\", \n",
        "    \"score\": \"100\", \n",
        "    \"sort-name\": \"Nirvana\", \n",
        "    \"tags\": [\n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"rock\"\n",
        "        }, \n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"pop\"\n",
        "        }, \n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"progressive rock\"\n",
        "        }, \n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"orchestral\"\n",
        "        }, \n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"british\"\n",
        "        }, \n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"power pop\"\n",
        "        }, \n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"psychedelic rock\"\n",
        "        }, \n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"soft rock\"\n",
        "        }, \n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"symphonic rock\"\n",
        "        }, \n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"english\"\n",
        "        }\n",
        "    ], \n",
        "    \"type\": \"Group\"\n",
        "}\n",
        "requesting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://musicbrainz.org/ws/2/artist/9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6?fmt=json&inc=releases\n",
        "\n",
        "ONE RELEASE:\n",
        "{\n",
        "  \"barcode\": null, \n",
        "  \"country\": \"GB\", \n",
        "  \"date\": \"1969\", \n",
        "  \"disambiguation\": \"\", \n",
        "  \"id\": \"0b44cb36-550a-491d-bfd9-8751271f9de7\", \n",
        "  \"packaging\": null, \n",
        "  \"quality\": \"normal\", \n",
        "  \"release-events\": [\n",
        "    {\n",
        "      \"area\": {\n",
        "        \"disambiguation\": \"\", \n",
        "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
        "        \"iso_3166_1_codes\": [\n",
        "          \"GB\"\n",
        "        ], \n",
        "        \"iso_3166_2_codes\": [], \n",
        "        \"iso_3166_3_codes\": [], \n",
        "        \"name\": \"United Kingdom\", \n",
        "        \"sort-name\": \"United Kingdom\"\n",
        "      }, \n",
        "      \"date\": \"1969\"\n",
        "    }\n",
        "  ], \n",
        "  \"status\": \"Official\", \n",
        "  \"text-representation\": {\n",
        "    \"language\": \"eng\", \n",
        "    \"script\": \"Latn\"\n",
        "  }, \n",
        "  \"title\": \"To Markos III\"\n",
        "}\n",
        "\n",
        "ALL TITLES:\n",
        "To Markos III\n",
        "Travelling on a Cloud\n",
        "Songs Of Love And Praise\n",
        "Songs of Love and Praise\n",
        "Songs of Love and Praise\n",
        "Secret Theatre\n",
        "The Story of Simon Simopath\n",
        "Me And My Friend\n",
        "All of Us\n",
        "The Story of Simon Simopath\n",
        "To Markos III\n",
        "Chemistry\n",
        "Local Anaesthetic\n",
        "Orange & Blue\n",
        "Pentecost Hotel\n",
        "Black Flower\n",
        "All of Us\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###Lesson 1: Exercise 4 JSON webqueries - answer required\n",
      "\n",
      "\n",
      "import json\n",
      "import requests\n",
      "\n",
      "\n",
      "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
      "ARTIST_URL = BASE_URL + \"artist/\"\n",
      "\n",
      "query_type = {  \"simple\": {},\n",
      "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
      "                \"aliases\": {\"inc\": \"aliases\"},\n",
      "                \"releases\": {\"inc\": \"releases\"}}\n",
      "\n",
      "\n",
      "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
      "    params[\"fmt\"] = fmt\n",
      "    r = requests.get(url + uid, params=params)\n",
      "    print \"requesting\", r.url\n",
      "\n",
      "    if r.status_code == requests.codes.ok:\n",
      "        return r.json()\n",
      "    else:\n",
      "        r.raise_for_status()\n",
      "\n",
      "\n",
      "def query_by_name(url, params, name):              #concat URL\n",
      "    params[\"query\"] = \"artist:\" + name\n",
      "    return query_site(url, params)\n",
      "\n",
      "\n",
      "def pretty_print(data, indent=4):                  #cuts up JSON\n",
      "    if type(data) == dict:\n",
      "        print json.dumps(data, indent=indent, sort_keys=True)\n",
      "    else:\n",
      "        print data\n",
      "\n",
      "\n",
      "def main():\n",
      "    global artist_id\n",
      "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"First Aid Kit\")\n",
      "    artist_id = results[\"artists\"][0][\"id\"]     #double indexing as array in dict value - this is the ID position, number is right result\n",
      "    \n",
      "    pretty_print(results)\n",
      "    \n",
      "    print \"\\n\\n\\nARTIST Unique ID:\"\n",
      "    pretty_print(artist_id)\n",
      "    \n",
      "    print \"\\n\\n\\nARTIST Overview:\"\n",
      "    pretty_print(results[\"artists\"][0])\n",
      "    \n",
      "    print \"\\n\\n\\nNumber of Bands with Same Name:\"\n",
      "    #pretty_print(results[\"count\"])\n",
      "    allnames = results[\"artists\"]\n",
      "    band_names = [r[\"name\"] for r in allnames]\n",
      "    print \"\\NAMES below:\"\n",
      "    for t in band_names:\n",
      "        print t\n",
      "    \n",
      "    #print \"\\n\\n\\nBegin-Area Name:\"\n",
      "    #pretty_print(results[\"artists\"][0][\"begin-area\"][\"name\"])\n",
      "    \n",
      "    #print \"\\n\\n\\nSpanish Alias Name:\"\n",
      "    #pretty_print(results[\"artists\"][0][\"aliases\"][8][\"name\"])\n",
      "\n",
      "    print \"\\n\\n\\nDisambiguation:\"\n",
      "    pretty_print(results[\"artists\"][0][\"disambiguation\"])\n",
      "    \n",
      "    print \"\\n\\n\\nFormed Date:\"\n",
      "    pretty_print(results[\"artists\"][0][\"life-span\"][\"begin\"])\n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3AFirst+Aid+Kit&fmt=json\n",
        "{\n",
        "    \"artists\": [\n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"23d10872-f5ae-3f0c-bf55-332788a16ecb\", \n",
        "                \"name\": \"Sweden\", \n",
        "                \"sort-name\": \"Sweden\"\n",
        "            }, \n",
        "            \"country\": \"SE\", \n",
        "            \"disambiguation\": \"Swedish folk band\", \n",
        "            \"id\": \"373faa02-74d7-4b1d-9b47-7574ad510f8d\", \n",
        "            \"life-span\": {\n",
        "                \"begin\": \"2007\", \n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"First Aid Kit\", \n",
        "            \"score\": \"100\", \n",
        "            \"sort-name\": \"First Aid Kit\", \n",
        "            \"tags\": [\n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"remember\"\n",
        "                }\n",
        "            ], \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"471c46a7-afc5-31c4-923c-d0444f5053a4\", \n",
        "                \"name\": \"Spain\", \n",
        "                \"sort-name\": \"Spain\"\n",
        "            }, \n",
        "            \"country\": \"ES\", \n",
        "            \"disambiguation\": \"Spanish indie electronic band\", \n",
        "            \"id\": \"e4466078-fb0f-4899-98a2-7c7dfeb14714\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"First Aid Kit\", \n",
        "            \"score\": \"100\", \n",
        "            \"sort-name\": \"First Aid Kit\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"1f1fc3a4-9500-39b8-9f10-f0a465557eef\", \n",
        "                \"name\": \"Russia\", \n",
        "                \"sort-name\": \"Russia\"\n",
        "            }, \n",
        "            \"country\": \"RU\", \n",
        "            \"disambiguation\": \"Russian metal\", \n",
        "            \"id\": \"5bb6d8b6-dcd8-46e0-84dc-5a0d927c8475\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"First Aid\", \n",
        "            \"score\": \"48\", \n",
        "            \"sort-name\": \"First Aid\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"b9f7d640-46e8-313e-b158-ded6d18593b3\", \n",
        "                \"name\": \"South Korea\", \n",
        "                \"sort-name\": \"South Korea\"\n",
        "            }, \n",
        "            \"country\": \"KR\", \n",
        "            \"disambiguation\": \"Korean person\", \n",
        "            \"gender\": \"male\", \n",
        "            \"id\": \"3266ea60-5690-464a-ae67-27c8681a9723\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"FIRST AID\", \n",
        "            \"score\": \"48\", \n",
        "            \"sort-name\": \"FIRST AID\", \n",
        "            \"type\": \"Person\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"e1c1215f-dcc0-35b4-b840-d2ca2151593b\", \n",
        "                \"name\": \"Estonia\", \n",
        "                \"sort-name\": \"Estonia\"\n",
        "            }, \n",
        "            \"country\": \"EE\", \n",
        "            \"disambiguation\": \"Estonian band\", \n",
        "            \"id\": \"93d871dc-38f4-442e-8292-db7d670794ea\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"First Aid\", \n",
        "            \"score\": \"48\", \n",
        "            \"sort-name\": \"First Aid\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"caac77d1-a5c8-3e6e-8e27-90b44dcc1446\", \n",
        "                \"name\": \"Austria\", \n",
        "                \"sort-name\": \"Austria\"\n",
        "            }, \n",
        "            \"begin-area\": {\n",
        "                \"id\": \"caac77d1-a5c8-3e6e-8e27-90b44dcc1446\", \n",
        "                \"name\": \"Austria\", \n",
        "                \"sort-name\": \"Austria\"\n",
        "            }, \n",
        "            \"country\": \"AT\", \n",
        "            \"disambiguation\": \"Austrian Rockband\", \n",
        "            \"id\": \"268d84a6-620a-4a87-b467-d2236f9264b6\", \n",
        "            \"life-span\": {\n",
        "                \"begin\": \"2000\", \n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"First Aid\", \n",
        "            \"score\": \"48\", \n",
        "            \"sort-name\": \"First Aid\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"c3eee377-bfd1-49fa-906c-7562db86a45d\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"The First Aid\", \n",
        "            \"score\": \"38\", \n",
        "            \"sort-name\": \"First Aid, The\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"612e0dcb-2b47-4feb-ad34-6259433de61d\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"First Aid 4 Souls\", \n",
        "            \"score\": \"38\", \n",
        "            \"sort-name\": \"First Aid 4 Souls\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"fbf39528-f7a1-491d-a5d4-94df61f88ad9\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"DJ First Aid\", \n",
        "            \"score\": \"38\", \n",
        "            \"sort-name\": \"DJ First Aid\"\n",
        "        }, \n",
        "        {\n",
        "            \"disambiguation\": \"Germany\", \n",
        "            \"id\": \"97b182ff-5f86-451c-b2b4-27840b7d512a\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"First Aid\", \n",
        "            \"score\": \"35\", \n",
        "            \"sort-name\": \"First Aid\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"ae609ba9-5df4-4ef4-aa4a-18805397ef24\", \n",
        "            \"life-span\": {\n",
        "                \"begin\": \"1990\", \n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"A\\u00edd\", \n",
        "            \"score\": \"31\", \n",
        "            \"sort-name\": \"A\\u00edd\", \n",
        "            \"type\": \"Person\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"ed6459f1-257f-433a-85c1-970b2fbbc0af\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"K.I.T.\", \n",
        "            \"score\": \"28\", \n",
        "            \"sort-name\": \"K.I.T.\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"2b1895a9-6906-4700-b0ed-98561cb58474\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Kit\", \n",
        "            \"score\": \"28\", \n",
        "            \"sort-name\": \"Kit\", \n",
        "            \"type\": \"Person\"\n",
        "        }, \n",
        "        {\n",
        "            \"disambiguation\": \"Kit Fegradoe - UK ambient/electronic artist\", \n",
        "            \"id\": \"b199b386-e0b1-4137-9faa-c9bc09b1eda4\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Kit\", \n",
        "            \"score\": \"28\", \n",
        "            \"sort-name\": \"Kit\"\n",
        "        }, \n",
        "        {\n",
        "            \"area\": {\n",
        "                \"id\": \"08310658-51eb-3801-80de-5a0739207115\", \n",
        "                \"name\": \"France\", \n",
        "                \"sort-name\": \"France\"\n",
        "            }, \n",
        "            \"country\": \"FR\", \n",
        "            \"disambiguation\": \"Paris electro-pop trio\", \n",
        "            \"id\": \"2d1caf76-ccd5-4cbc-a1b0-c36186c81c15\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"KIT\", \n",
        "            \"score\": \"28\", \n",
        "            \"sort-name\": \"KIT\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"disambiguation\": \"George Chen, Vice Cooler\", \n",
        "            \"id\": \"ede72177-00cd-4ddf-b04b-1b7c754fed79\", \n",
        "            \"life-span\": {\n",
        "                \"begin\": \"2003\", \n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"KIT\", \n",
        "            \"score\": \"26\", \n",
        "            \"sort-name\": \"KIT\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"aliases\": [\n",
        "                {\n",
        "                    \"begin-date\": null, \n",
        "                    \"end-date\": null, \n",
        "                    \"locale\": null, \n",
        "                    \"name\": \"Ferry Aid (Zeebrugge)\", \n",
        "                    \"primary\": null, \n",
        "                    \"sort-name\": \"Ferry Aid (Zeebrugge)\", \n",
        "                    \"type\": null\n",
        "                }\n",
        "            ], \n",
        "            \"id\": \"590233d0-5ab2-4a67-8f43-a647c2100bdc\", \n",
        "            \"life-span\": {\n",
        "                \"begin\": \"1987\", \n",
        "                \"end\": \"1987\", \n",
        "                \"ended\": true\n",
        "            }, \n",
        "            \"name\": \"Ferry Aid\", \n",
        "            \"score\": \"20\", \n",
        "            \"sort-name\": \"Ferry Aid\", \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"0ecaa896-58fa-4dca-b53d-8da7bc5f59c5\", \n",
        "            \"life-span\": {\n",
        "                \"begin\": \"1984\", \n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Band Aid\", \n",
        "            \"score\": \"19\", \n",
        "            \"sort-name\": \"Band Aid\", \n",
        "            \"tags\": [\n",
        "                {\n",
        "                    \"count\": 1, \n",
        "                    \"name\": \"british\"\n",
        "                }\n",
        "            ], \n",
        "            \"type\": \"Group\"\n",
        "        }, \n",
        "        {\n",
        "            \"gender\": \"male\", \n",
        "            \"id\": \"d8e4fd9f-0a1b-4dbe-9ea6-8352d885fd84\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Peter Aid\", \n",
        "            \"score\": \"19\", \n",
        "            \"sort-name\": \"Aid, Peter\", \n",
        "            \"type\": \"Person\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"d74731bd-79aa-4452-876d-b25d5c78b0b2\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Quench Aid\", \n",
        "            \"score\": \"19\", \n",
        "            \"sort-name\": \"Quench Aid\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"d31ee795-51b3-4213-81ff-ea521f988d28\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Hearin Aid\", \n",
        "            \"score\": \"19\", \n",
        "            \"sort-name\": \"Hearin Aid\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"375093c9-b20a-4955-80d1-e84fbacf4a6b\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Kool Aid\", \n",
        "            \"score\": \"19\", \n",
        "            \"sort-name\": \"Kool Aid\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"d2559845-4898-458e-aacf-6392ef92dc19\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Mint-Aid\", \n",
        "            \"score\": \"19\", \n",
        "            \"sort-name\": \"Mint-Aid\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"7fe6ce13-78e4-4dc0-9820-aa2f4acd844a\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Punk Aid\", \n",
        "            \"score\": \"19\", \n",
        "            \"sort-name\": \"Punk Aid\"\n",
        "        }, \n",
        "        {\n",
        "            \"id\": \"fae0d35f-ce2a-42e1-a0e5-b315fbb5ff0a\", \n",
        "            \"life-span\": {\n",
        "                \"ended\": null\n",
        "            }, \n",
        "            \"name\": \"Whale Aid\", \n",
        "            \"score\": \"19\", \n",
        "            \"sort-name\": \"Whale Aid\", \n",
        "            \"type\": \"Group\"\n",
        "        }\n",
        "    ], \n",
        "    \"count\": 358, \n",
        "    \"created\": \"2015-01-21T02:39:57.307Z\", \n",
        "    \"offset\": 0\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "ARTIST Unique ID:\n",
        "373faa02-74d7-4b1d-9b47-7574ad510f8d\n",
        "\n",
        "\n",
        "\n",
        "ARTIST Overview:\n",
        "{\n",
        "    \"area\": {\n",
        "        \"id\": \"23d10872-f5ae-3f0c-bf55-332788a16ecb\", \n",
        "        \"name\": \"Sweden\", \n",
        "        \"sort-name\": \"Sweden\"\n",
        "    }, \n",
        "    \"country\": \"SE\", \n",
        "    \"disambiguation\": \"Swedish folk band\", \n",
        "    \"id\": \"373faa02-74d7-4b1d-9b47-7574ad510f8d\", \n",
        "    \"life-span\": {\n",
        "        \"begin\": \"2007\", \n",
        "        \"ended\": null\n",
        "    }, \n",
        "    \"name\": \"First Aid Kit\", \n",
        "    \"score\": \"100\", \n",
        "    \"sort-name\": \"First Aid Kit\", \n",
        "    \"tags\": [\n",
        "        {\n",
        "            \"count\": 1, \n",
        "            \"name\": \"remember\"\n",
        "        }\n",
        "    ], \n",
        "    \"type\": \"Group\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "Number of Bands with Same Name:\n",
        "\\NAMES below:\n",
        "First Aid Kit\n",
        "First Aid Kit\n",
        "First Aid\n",
        "FIRST AID\n",
        "First Aid\n",
        "First Aid\n",
        "The First Aid\n",
        "First Aid 4 Souls\n",
        "DJ First Aid\n",
        "First Aid\n",
        "A\u00edd\n",
        "K.I.T.\n",
        "Kit\n",
        "Kit\n",
        "KIT\n",
        "KIT\n",
        "Ferry Aid\n",
        "Band Aid\n",
        "Peter Aid\n",
        "Quench Aid\n",
        "Hearin Aid\n",
        "Kool Aid\n",
        "Mint-Aid\n",
        "Punk Aid\n",
        "Whale Aid\n",
        "\n",
        "\n",
        "\n",
        "Disambiguation:\n",
        "Swedish folk band\n",
        "\n",
        "\n",
        "\n",
        "Formed Date:\n",
        "2007\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##LESSON 1: Problem Set Q1\n",
      "\n",
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Your task is to process the supplied file and use the csv module to extract data from it.\n",
      "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
      "contains information from one meteorological station, in particular - about amount of\n",
      "solar and wind energy for each hour of day.\n",
      "\n",
      "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
      "describing the data source. You should extract the name of the station from it.\n",
      "\n",
      "The data should be returned as a list of lists (not dictionaries).\n",
      "You can use the csv modules \"reader\" method to get data in such format.\n",
      "Another useful method is next() - to get the next line from the iterator.\n",
      "You should only change the parse_file function.\n",
      "\"\"\"\n",
      "import csv\n",
      "import os\n",
      "\n",
      "DATADIR = \"/Users/richardshanahan/Documents/DATA SCIENCE/99. Self Study/Udacity/MongoDB Data Wrangling/99. Downloads/\"\n",
      "DATAFILE = \"745090TY.csv\"\n",
      "\n",
      "#csvreader.next()\n",
      "\n",
      "def parse_file(datafile):\n",
      "    global data\n",
      "    global reader\n",
      "    name = \"\"\n",
      "    data = []\n",
      "    j = 1\n",
      "    with open(datafile,'rb') as f:\n",
      "     \n",
      "        reader = csv.reader(f)\n",
      "\n",
      "        ###first answer\n",
      "        #for line in f:\n",
      "        #    reader = csv.reader(f)\n",
      "        #    data = list(next(reader))\n",
      "        #pass\n",
      "        \n",
      "        ###second submitted answer\n",
      "        #for line in reader:\n",
      "        #    if (j==1):\n",
      "        #       name=line[1]\n",
      "        #       print name\n",
      "        #    if(j==2):\n",
      "        #        pass\n",
      "        #    if(j>=3):\n",
      "        #        data.append(line)\n",
      "            \n",
      "        #    j = j+1\n",
      "        \n",
      "        ###Udacity answer\n",
      "        name = \"\"\n",
      "        data = []\n",
      "        with open(datafile,'rb') as f:\n",
      "            r = csv.reader(f)\n",
      "            name = r.next()[1]\n",
      "            header = r.next()\n",
      "            data = [row for row in r]\n",
      "        \n",
      "    # Do not change the line below\n",
      "    return (name, data)\n",
      "\n",
      "\n",
      "def test():\n",
      "    datafile = os.path.join(DATADIR, DATAFILE)\n",
      "    name, data = parse_file(datafile)\n",
      "\n",
      "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
      "    assert data[0][1] == \"01:00\"\n",
      "    assert data[2][0] == \"01/01/2005\"\n",
      "    assert data[2][5] == \"2\"\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MOUNTAIN VIEW MOFFETT FLD NAS\n"
       ]
      }
     ],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##LESSON 1: Problem Set Q2 - MY ANSWER\n",
      "\n",
      "\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "# Find the time and value of max load for each of the regions\n",
      "# COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
      "# and write the result out in a csv file, using pipe character | as the delimiter.\n",
      "# An example output can be seen in the \"example.csv\" file.\n",
      "\n",
      "import xlrd\n",
      "import os\n",
      "import csv\n",
      "from zipfile import ZipFile\n",
      "import numpy as np\n",
      "\n",
      "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
      "outfile = \"2013_Max_Loads.csv\"\n",
      "\n",
      "\n",
      "def open_zip(datafile):\n",
      "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
      "        myzip.extractall()\n",
      "\n",
      "\n",
      "def parse_file(datafile):\n",
      "    global data\n",
      "    workbook = xlrd.open_workbook(datafile)\n",
      "    sheet = workbook.sheet_by_index(0)\n",
      "    data = []\n",
      "    \n",
      "    # YOUR CODE HERE\n",
      "    headerl = list([\"Station\",\"Year\",\"Month\",\"Day\",\"Hour\",\"Max Load\"])\n",
      "    header = dict(enumerate(headerl))\n",
      "    \n",
      "    \n",
      "    #read time column\n",
      "    times_float = sheet.col_values(0, start_rowx=1)\n",
      "\n",
      "    #read values for each region\n",
      "    coast = sheet.col_values(1,start_rowx=1)\n",
      "    east = sheet.col_values(2,start_rowx=1)\n",
      "    far_west = sheet.col_values(3,start_rowx=1)\n",
      "    north = sheet.col_values(4,start_rowx=1)\n",
      "    north_c = sheet.col_values(5,start_rowx=1)\n",
      "    southern = sheet.col_values(6,start_rowx=1)\n",
      "    south_c = sheet.col_values(7,start_rowx=1)\n",
      "    west = sheet.col_values(8,start_rowx=1)\n",
      "    \n",
      "\n",
      "    #index of the max value\n",
      "    idx_max_coast = coast.index(max(coast))    \n",
      "    idx_max_east = east.index(max(east))\n",
      "    idx_max_far_west = far_west.index(max(far_west))\n",
      "    idx_max_north = north.index(max(north))\n",
      "    idx_max_north_c = north_c.index(max(north_c))\n",
      "    idx_max_southern = southern.index(max(southern))\n",
      "    idx_max_south_c = south_c.index(max(south_c))\n",
      "    idx_max_west = west.index(max(west))\n",
      "\n",
      "    #writer = csv.writer(csvfile, delimiter='|')\n",
      "    \n",
      "    #data = {\n",
      "    #        'maxvalue': round(max(sheet.col_values(1, start_rowx=1)), 10),\n",
      "    #        'maxtime': xlrd.xldate_as_tuple(times_float[idx_max],0)\n",
      "    #        }\n",
      "    \n",
      "    data.append(str(header.values()).strip('[]'))\n",
      "    data.append(str([sheet.col_values(1,start_rowx=0,end_rowx=1), (xlrd.xldate_as_tuple(times_float[idx_max_coast],0)), (round(max(coast), 10))]).strip('[]'))\n",
      "    data.append(str([sheet.col_values(2,start_rowx=0,end_rowx=1), (xlrd.xldate_as_tuple(times_float[idx_max_east],0)), (round(max(east), 10))]).strip('[]'))\n",
      "    data.append(str([sheet.col_values(3,start_rowx=0,end_rowx=1), (xlrd.xldate_as_tuple(times_float[idx_max_far_west],0)), (round(max(far_west), 10))]).strip('[]'))\n",
      "    data.append(str([sheet.col_values(4,start_rowx=0,end_rowx=1), (xlrd.xldate_as_tuple(times_float[idx_max_north],0)), (round(max(north), 10))]).strip('[]'))\n",
      "    data.append(str([sheet.col_values(5,start_rowx=0,end_rowx=1), (xlrd.xldate_as_tuple(times_float[idx_max_north_c],0)), (round(max(north_c), 10))]).strip('[]'))\n",
      "    data.append(str([sheet.col_values(6,start_rowx=0,end_rowx=1), (xlrd.xldate_as_tuple(times_float[idx_max_southern],0)), (round(max(southern), 10))]).strip('[]'))\n",
      "    data.append(str([sheet.col_values(7,start_rowx=0,end_rowx=1), (xlrd.xldate_as_tuple(times_float[idx_max_south_c],0)), (round(max(south_c), 10))]).strip('[]'))\n",
      "    data.append(str([sheet.col_values(8,start_rowx=0,end_rowx=1), (xlrd.xldate_as_tuple(times_float[idx_max_west],0)), (round(max(west), 10))]).strip('[]'))\n",
      "\n",
      "    \n",
      "    \n",
      "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
      "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
      "    return data\n",
      "\n",
      "#def save_file(data, filename):\n",
      "    # YOUR CODE HERE\n",
      "\n",
      "    \n",
      "def test():\n",
      "    #open_zip(datafile)\n",
      "    data = parse_file(datafile)\n",
      "    #save_file(data, outfile)\n",
      "\n",
      "    number_of_rows = 0\n",
      "    stations = []\n",
      "\n",
      "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
      "                        'Year': '2013',\n",
      "                        'Month': '6',\n",
      "                        'Day': '26',\n",
      "                        'Hour': '17'}}\n",
      "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
      "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
      "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
      "\n",
      "    with open(outfile) as of:\n",
      "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
      "        for line in csvfile:\n",
      "            station = line['Station']\n",
      "            if station == 'FAR_WEST':\n",
      "                for field in fields:\n",
      "                    # Check if 'Max Load' is within .1 of answer\n",
      "                    if field == 'Max Load':\n",
      "                        max_answer = round(float(ans[station][field]), 1)\n",
      "                        max_line = round(float(line[field]), 1)\n",
      "                        assert max_answer == max_line\n",
      "\n",
      "                    # Otherwise check for equality\n",
      "                    else:\n",
      "                        assert ans[station][field] == line[field]\n",
      "\n",
      "            number_of_rows += 1\n",
      "            stations.append(station)\n",
      "\n",
      "        # Output should be 8 lines not including header\n",
      "        assert number_of_rows == 8\n",
      "\n",
      "        # Check Station Names\n",
      "        assert set(stations) == set(correct_stations)\n",
      "\n",
      "        \n",
      "if __name__ == \"__main__\":\n",
      "    test()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: '2013_Max_Loads.csv'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-248-64332c44917f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-248-64332c44917f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hour'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Max Load'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mcsvfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '2013_Max_Loads.csv'"
       ]
      }
     ],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##LESSON 1: Problem Set Q2 - UDACITY ANSWER\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "# Find the time and value of max load for each of the regions\n",
      "# COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
      "# and write the result out in a csv file, using pipe character | as the delimiter.\n",
      "# An example output can be seen in the \"example.csv\" file.\n",
      "\n",
      "import xlrd\n",
      "import os\n",
      "import csv\n",
      "from zipfile import ZipFile\n",
      "\n",
      "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
      "outfile = \"2013_Max_Loads.csv\"\n",
      "\n",
      "\n",
      "def open_zip(datafile):\n",
      "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
      "        myzip.extractall()\n",
      "\n",
      "\n",
      "def parse_file(datafile):\n",
      "    global data\n",
      "    workbook = xlrd.open_workbook(datafile)\n",
      "    sheet = workbook.sheet_by_index(0)\n",
      "    data = {}\n",
      "    # process all rows that contain station data\n",
      "    for n in range (1, 9):\n",
      "        station = sheet.cell_value(0, n)\n",
      "        cv = sheet.col_values(n, start_rowx=1, end_rowx=None)\n",
      "\n",
      "        maxval = max(cv)\n",
      "        maxpos = cv.index(maxval) + 1\n",
      "        maxtime = sheet.cell_value(maxpos, 0)\n",
      "        realtime = xlrd.xldate_as_tuple(maxtime, 0)\n",
      "        data[station] = {\"maxval\": maxval,\n",
      "                         \"maxtime\": realtime}\n",
      "\n",
      "    print data\n",
      "    return data\n",
      "\n",
      "def save_file(data, filename):\n",
      "    with open(filename, \"w\") as f:\n",
      "        w = csv.writer(f, delimiter='|')\n",
      "        w.writerow([\"Station\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Max Load\"])\n",
      "        for s in data:\n",
      "            year, month, day, hour, _ , _= data[s][\"maxtime\"]\n",
      "            w.writerow([s, year, month, day, hour, data[s][\"maxval\"]])\n",
      "    \n",
      "def test():\n",
      "    #open_zip(datafile)\n",
      "    data = parse_file(datafile)\n",
      "    save_file(data, outfile)\n",
      "\n",
      "    number_of_rows = 0\n",
      "    stations = []\n",
      "\n",
      "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
      "                        'Year': '2013',\n",
      "                        'Month': '6',\n",
      "                        'Day': '26',\n",
      "                        'Hour': '17'}}\n",
      "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
      "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
      "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
      "\n",
      "    with open(outfile) as of:\n",
      "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
      "        for line in csvfile:\n",
      "            station = line['Station']\n",
      "            if station == 'FAR_WEST':\n",
      "                for field in fields:\n",
      "                    # Check if 'Max Load' is within .1 of answer\n",
      "                    if field == 'Max Load':\n",
      "                        max_answer = round(float(ans[station][field]), 1)\n",
      "                        max_line = round(float(line[field]), 1)\n",
      "                        assert max_answer == max_line\n",
      "\n",
      "                    # Otherwise check for equality\n",
      "                    else:\n",
      "                        assert ans[station][field] == line[field]\n",
      "\n",
      "            number_of_rows += 1\n",
      "            stations.append(station)\n",
      "\n",
      "        # Output should be 8 lines not including header\n",
      "        assert number_of_rows == 8\n",
      "\n",
      "        # Check Station Names\n",
      "        assert set(stations) == set(correct_stations)\n",
      "\n",
      "        \n",
      "if __name__ == \"__main__\":\n",
      "    test()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'FAR_WEST': {'maxval': 2281.2722140000024, 'maxtime': (2013, 6, 26, 17, 0, 0)}, u'NORTH': {'maxval': 1544.7707140000005, 'maxtime': (2013, 8, 7, 17, 0, 0)}, u'WEST': {'maxval': 1862.6137649999998, 'maxtime': (2013, 8, 7, 17, 0, 0)}, u'SOUTHERN': {'maxval': 5494.157645, 'maxtime': (2013, 8, 8, 16, 0, 0)}, u'SOUTH_C': {'maxval': 11433.30491600001, 'maxtime': (2013, 8, 8, 18, 0, 0)}, u'COAST': {'maxval': 18779.025510000003, 'maxtime': (2013, 8, 13, 17, 0, 0)}, u'NORTH_C': {'maxval': 24415.570226999993, 'maxtime': (2013, 8, 7, 18, 0, 0)}, u'EAST': {'maxval': 2380.1654089999956, 'maxtime': (2013, 8, 5, 17, 0, 0)}}\n"
       ]
      }
     ],
     "prompt_number": 269
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##LESSON 1: Problem Set Q3 - MY ANSWER PASSES\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "This exercise shows some important concepts that you should be aware about:\n",
      "- using codecs module to write unicode files\n",
      "- using authentication with web APIs\n",
      "- using offset when accessing web APIs\n",
      "\n",
      "To run this code locally you have to register at the NYTimes developer site \n",
      "and get your own API key. You will be able to complete this exercise in our UI without doing so,\n",
      "as we have provided a sample result.\n",
      "\n",
      "Your task is to process the saved file that represents the most popular (by view count)\n",
      "articles in the last day, and return the following data:\n",
      "- list of dictionaries, where the dictionary key is \"section\" and value is \"title\"\n",
      "- list of URLs for all media entries with \"format\": \"Standard Thumbnail\"\n",
      "\n",
      "All your changes should be in the article_overview function.\n",
      "The rest of functions are provided for your convenience, if you want to access the API by yourself.\n",
      "\"\"\"\n",
      "import json\n",
      "import codecs\n",
      "import requests\n",
      "\n",
      "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
      "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
      "API_KEY = { \"popular\": \"d5421b6210edd4ccabf27866ecd3deda:17:70676280\",\n",
      "            \"article\": \"024e7335f9bfd7c727f1c6c4024f23ae:6:70676280\"}\n",
      "\n",
      "\n",
      "def get_from_file(kind, period):\n",
      "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
      "    with open(filename, \"r\") as f:\n",
      "        return json.loads(f.read())\n",
      "\n",
      "\n",
      "def article_overview(kind, period):\n",
      "    data = get_from_file(kind, period)\n",
      "    titles = []\n",
      "    urls =[]\n",
      "    # YOUR CODE HERE\n",
      "\n",
      "       \n",
      "    for i in range(len(data)):\n",
      "        minidict = {}\n",
      "        minidict[data[i][\"section\"]] = data[i][\"title\"]\n",
      "        titles.append(minidict)\n",
      "    \n",
      "    for j in range(len(data)):\n",
      "        for k in data[j][\"media\"]:\n",
      "            urls.append(k['media-metadata'][0]['url'])\n",
      "        \n",
      "    \n",
      "    return (titles, urls)\n",
      "\n",
      "\n",
      "def query_site(url, target, offset):\n",
      "    # This will set up the query with the API key and offset\n",
      "    # Web services often use offset paramter to return data in small chunks\n",
      "    # NYTimes returns 20 articles per request, if you want the next 20\n",
      "    # You have to provide the offset parameter\n",
      "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
      "        print \"You need to register for NYTimes Developer account to run this program.\"\n",
      "        print \"See Intructor notes for information\"\n",
      "        return False\n",
      "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
      "    r = requests.get(url, params = params)\n",
      "\n",
      "    if r.status_code == requests.codes.ok:\n",
      "        return r.json()\n",
      "    else:\n",
      "        r.raise_for_status()\n",
      "\n",
      "\n",
      "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
      "    # This function will construct the query according to the requirements of the site\n",
      "    # and return the data, or print an error message if called incorrectly\n",
      "    if days not in [1,7,30]:\n",
      "        print \"Time period can be 1,7, 30 days only\"\n",
      "        return False\n",
      "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
      "        print \"kind can be only one of viewed/shared/emailed\"\n",
      "        return False\n",
      "\n",
      "    url = URL_POPULAR + \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
      "    data = query_site(url, \"popular\", offset)\n",
      "\n",
      "    return data\n",
      "\n",
      "\n",
      "def save_file(kind, period):\n",
      "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
      "    # combine the data and then write all results in a file.\n",
      "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
      "    num_results = data[\"num_results\"]\n",
      "    full_data = []\n",
      "    with codecs.open(\"popular-{0}-{1}-full.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
      "        for offset in range(0, num_results, 20):        \n",
      "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
      "            full_data += data[\"results\"]\n",
      "        \n",
      "        v.write(json.dumps(full_data, indent=2))\n",
      "\n",
      "\n",
      "def test():\n",
      "    titles, urls = article_overview(\"viewed\", 1)\n",
      "    assert len(titles) == 20\n",
      "    assert len(urls) == 30\n",
      "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
      "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    }
   ],
   "metadata": {}
  }
 ]
}
